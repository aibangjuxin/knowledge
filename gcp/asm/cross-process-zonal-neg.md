ä½ å¸®æˆ‘å»æ¢ç´¢ ä¸»é¢˜
åœ¨ Shared VPC ç¯å¢ƒä¸‹ Internal HTTPS Load Balancer è·¨é¡¹ç›®ç»‘å®š Backend çš„å¯è¡Œæ€§ç¡®è®¤
â¸»
èƒŒæ™¯è¯´æ˜
æˆ‘ä»¬æ­£åœ¨ Google Cloud ä¸Šè®¾è®¡ä¸€ä¸ªå¤šç§Ÿæˆ·æ¶æ„ï¼Œå¸Œæœ›ç¡®è®¤è·¨é¡¹ç›® Backend ç»‘å®šçš„å®˜æ–¹æ”¯æŒæ–¹å¼ã€‚
å½“å‰æ¶æ„
	â€¢	Tenant é¡¹ç›® ä¸»è¦æ˜¯æƒ³è¦ç”¨æˆ·æ§åˆ¶è‡ªå·±çš„å…¥å£ ç»‘å®šè‡ªå·±å¯¹åº”çš„è§„åˆ™ æ¯”å¦‚è¯´å…¶å¯¹åº”çš„cloud armor è§„åˆ™çš„æ”¹åŠ¨ï¼Œä¸ä¼šå½±å“æˆ‘ä»¬å…¶ä»–çš„ç”¨æˆ·ä¹Ÿå¯ä»¥æ”¯æŒè‡ªå·±çš„ç‹¬ç«‹æœºä¼š å®‰å…¨éšæ‚£åˆ†æ‘Šåˆ°tenant
	â€¢	Internal HTTPS Load Balancerï¼ˆINTERNAL_MANAGEDï¼‰
	â€¢	URL Map + Routing Rules
	â€¢	Backend Serviceï¼ˆå½’ Tenant é¡¹ç›®æ‰€æœ‰ï¼‰
	â€¢	Cloud Armor ç»‘å®šåœ¨ Backend Service ä¸Š
	â€¢	TLS è¯ä¹¦åœ¨ Tenant é¡¹ç›®ä¸­ç®¡ç†
	â€¢	Master é¡¹ç›® è¿™ä¸ªä¹Ÿæ˜¯æˆ‘ä»¬å¹³å°æ–¹åšä¸€ä¸ªæ ¸å¿ƒæ¥æä¾›ä¸€äº›å¯¹åº”çš„GKE
	â€¢	Managed Instance Groupï¼ˆMIGï¼‰ GKEÂ  neg
	â€¢	è¿è¡Œ Compute Engine VM å·¥ä½œè´Ÿè½½ vm é€šè¿‡ä¸€å®šçš„æ–¹å¼æ¥æš´éœ²æˆ‘ä»¬çš„GKE
	â€¢	ä¸¤ä¸ªé¡¹ç›®å·²æŒ‚è½½åœ¨åŒä¸€ä¸ª Shared VPCï¼ˆåŒä¸€ Host Project + åŒä¸€ VPCï¼‰
â¸»

ç›®æ ‡æ¶æ„

æˆ‘ä»¬å¸Œæœ›ï¼š
	â€¢	ä¿æŒ Internal HTTPS LB å’Œ Backend Service åœ¨ Tenant é¡¹ç›®
Â Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  æˆ‘ä»¬å·²ç»å®ç°äº†ï¼Œé€šè¿‡ä¿®æ”¹tenant ilb å¯ä»¥ç»‘å®šåˆ°cross projectåˆ°masterçš„ backend service .å½“ç„¶è™½ç„¶è¿™ä¸ªå·²ç»å®ç°äº†ï¼Œ æˆ‘ä»¬ä¹Ÿå¯ä»¥é’ˆå¯¹æ¯ä¸ªtalentè¿‡æ¥çš„ç”¨æˆ·åˆ›å»ºè‡ªå·±å¯¹åº”çš„backend service ä½†æ˜¯æˆ‘å¯èƒ½è¿˜è¦è¯„ä¼°ä¸€ä¸‹å¯¹åº”çš„å·¥ç¨‹çš„æˆæƒæƒ…å†µè¦åšå®‰å…¨è¯„ä¼°ï¼Œç°åœ¨åªæ˜¯å®ç°äº†è®¿é—®
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  æ”¾åœ¨tenantå¥½åƒå¿…é¡»å¾—ç»™tenant projectçš„sa compuer.instanseçš„adm
	â€¢	å°† Tenant é¡¹ç›®çš„ Backend Service ç»‘å®šåˆ° Master é¡¹ç›®ä¸­çš„ MIG æˆ–è€…neg GKE
	â€¢	Cloud Armor ä¾ç„¶åœ¨ Tenant é¡¹ç›®ä¾§ç”Ÿæ•ˆ
	â€¢	é€šè¿‡ Shared VPC å®ç°è·¨é¡¹ç›®ç½‘ç»œå…±äº«

å¸Œæœ›ç¡®è®¤çš„é—®é¢˜
	1.	åœ¨ Shared VPC æ¡ä»¶ä¸‹ï¼ŒBackend Serviceï¼ˆProject Aï¼‰ç»‘å®š MIGï¼ˆProject Bï¼‰æ˜¯å¦å±äºå®˜æ–¹æ”¯æŒæ¶æ„ï¼Ÿ
	2.	æ˜¯å¦å­˜åœ¨ä»¥ä¸‹æ–¹é¢çš„é™åˆ¶æˆ–æœ€ä½³å®è·µï¼š
	â€¢	è·¨é¡¹ç›® IAM æƒé™è¦æ±‚
	â€¢	Health Check çš„å½’å±å’Œå¯è§æ€§
	â€¢	Backend ç”Ÿå‘½å‘¨æœŸç®¡ç†å½±å“
	â€¢	Region é™åˆ¶
	3.	åœ¨ç”Ÿäº§ç¯å¢ƒä¸­æ˜¯å¦å­˜åœ¨å·²çŸ¥é£é™©æˆ–è¾¹ç•Œæƒ…å†µï¼Ÿ
	4.	ä» Google æ¨èæ¶æ„è§’åº¦ï¼Œæ˜¯å¦æ›´æ¨èï¼š
	â€¢	è·¨é¡¹ç›® MIG ç»‘å®š ä»¥åŠè¯¥æ–¹æ¡ˆæ˜¯å¦å¯è¡Œï¼Ÿ
	â€¢	è·¨é¡¹ç›® NEG ç»‘å®šä»¥åŠè¯¥æ–¹æ¡ˆæ˜¯å¦å¯è¡Œï¼Ÿ
	â€¢	æˆ–ä½¿ç”¨ Private Service Connect å®ç°æ›´å¼ºéš”ç¦»
å¦‚æœè¿™ç§æ–¹æ¡ˆå¯è¡Œçš„è¯ï¼Œæˆ‘ä»¬è¿˜éœ€è¦è€ƒè™‘ä¸€ä¸‹ä¸€äº›å®‰å…¨çš„è®¾ç½®ï¼Œæˆ–è€…æ˜¯å¯¹åº”å·¥ç¨‹æˆæƒçš„ä¸€äº›é—®é¢˜ä»¥åŠå¯¹åº”çš„ä¸€äº›è®¡è´¹åŸåˆ™ï¼Œæˆ‘ä»¬æƒ³å¯¹tenantçš„ç”¨æˆ·è¿›è¡Œç‹¬ç«‹è®¡è´¹ï¼Œä¹Ÿè¦è€ƒè™‘è®¡è´¹çš„æ–¹ä¾¿æ€§
æˆ‘ä»¬å¸Œæœ›åœ¨è¿›å…¥ç”Ÿäº§ç¯å¢ƒå‰ç¡®è®¤è¯¥è®¾è®¡ç¬¦åˆ Google Cloud å®˜æ–¹æ”¯æŒçš„æ¶æ„æ¨¡å¼ã€‚
æ‰€ä»¥æˆ‘å¸Œæœ›ä½ è‡³äºä¸Šé¢çš„è¿™ä¸ªèƒŒæ™¯ï¼Œæˆ‘å»æ¢ç´¢ä¸€ä¸‹æ˜¯å¦èƒ½å¤Ÿå®ç°
ç›®å‰æœ‰ä¸¤ç§æ¶æ„å®ç°æ–¹æ¡ˆå¯ä¾›è¯„ä¼°ï¼š
æ–¹æ¡ˆä¸€æ˜¯å°† Backend Server éƒ¨ç½²åœ¨ Talent é¡¹ç›®ä¸­ï¼›æ–¹æ¡ˆäºŒï¼ˆå³æˆ‘å½“å‰çš„å®ç°ï¼‰æ˜¯å°† Backend Server ç›´æ¥éƒ¨ç½²åœ¨ Master é¡¹ç›®ä¸‹ã€‚æ–¹æ¡ˆ B ä¸æ˜¯â€œå…±ç”¨ä¸€ä¸ª Backend Serviceâ€ï¼Œè€Œæ˜¯â€œæ¯ä¸ª Tenant ä¸€ä¸ª Backend Serviceï¼Œä½†éƒ½åœ¨ Master é¡¹ç›®â€ã€‚
é™¤äº†å®‰å…¨æ€§ä¹‹å¤–ï¼Œæˆ‘é‡ç‚¹å…³æ³¨æƒé™ç®¡æ§ï¼ˆAuthorizationï¼‰ä¸è®¡è´¹æ¶æ„ï¼ˆBillingï¼‰ã€‚
è‹¥é‡‡ç”¨å…¨é‡éƒ¨ç½²åœ¨ Master é¡¹ç›®çš„æ–¹æ¡ˆï¼Œæ‰€æœ‰ Cloud Armor çš„è®¡è´¹è´¹ç”¨å°†ç”± Master é¡¹ç›®æ‰¿æ‹…ã€‚ä¸”ç”±äºæˆ‘éœ€è¦åœ¨ Master é¡¹ç›®ä¸­åˆ›å»ºå¤šä¸ªä¸åŒçš„ Backend Serverï¼Œå„æœåŠ¡çš„è®¡è´¹ç»´åº¦ä¹Ÿéœ€è¦åšè¿›ä¸€æ­¥çš„æ‹†åˆ†ã€‚


# Shared VPC è·¨é¡¹ç›® Internal HTTPS LB ç»‘å®š Backend å¯è¡Œæ€§åˆ†æ

## ğŸ” é—®é¢˜åˆ†æ

æ ¸å¿ƒé—®é¢˜æ˜¯åœ¨ Shared VPC ç¯å¢ƒä¸‹ï¼Œè¯„ä¼°ä¸¤ç§æ¶æ„æ–¹æ¡ˆçš„**å¯è¡Œæ€§ã€æƒé™æ¨¡å‹ã€è®¡è´¹æ‹†åˆ†**ï¼š

- **æ–¹æ¡ˆ A**ï¼šBackend Service åœ¨ Tenant é¡¹ç›®ï¼Œç»‘å®š Master é¡¹ç›®çš„ MIG/NEGï¼ˆè·¨é¡¹ç›® Backend ç»‘å®šï¼‰
- **æ–¹æ¡ˆ B**ï¼šBackend Service åœ¨ Master é¡¹ç›®ï¼Œæ¯ä¸ª Tenant ç‹¬ç«‹ä¸€ä¸ª Backend Serviceï¼ˆå½“å‰å·²å®ç°ï¼‰

---

## ğŸ›  å¯è¡Œæ€§ç¡®è®¤

### æ–¹æ¡ˆ Aï¼šBackend Serviceï¼ˆTenantï¼‰â†’ MIG/NEGï¼ˆMasterï¼‰è·¨é¡¹ç›®ç»‘å®š

#### âœ… MIG è·¨é¡¹ç›®ç»‘å®š

| ç»´åº¦ | ç»“è®º |
|------|------|
| å®˜æ–¹æ”¯æŒ | **æœ‰é™æ”¯æŒ**ï¼Œéœ€ Shared VPC ä¸”æ»¡è¶³ IAM æ¡ä»¶ |
| ç½‘ç»œå±‚ | Shared VPC åŒ VPC ç½‘ç»œä¸‹ **å¯è¾¾** |
| GCP API å±‚ | Backend Service å¼•ç”¨è·¨é¡¹ç›® MIG éœ€è¦æ˜¾å¼ IAM æˆæƒ |
| Health Check | **å¿…é¡»ä¸ Backend Service åŒé¡¹ç›®**ï¼Œæˆ–æ˜ç¡®æˆæƒ |

**å…³é”® IAM æˆæƒè¦æ±‚ï¼ˆæ–¹æ¡ˆ A MIGï¼‰**ï¼š

```bash
# Tenant é¡¹ç›®çš„ Cloud Load Balancing Service Agent éœ€è¦è®¿é—® Master é¡¹ç›®çš„ MIG
# åœ¨ Master é¡¹ç›®æˆæƒ
gcloud projects add-iam-policy-binding <MASTER_PROJECT_ID> \
  --member="serviceAccount:service-<TENANT_PROJECT_NUMBER>@compute-system.iam.gserviceaccount.com" \
  --role="roles/compute.networkViewer"

# åŒæ—¶éœ€è¦æˆæƒè®¿é—® MIG
gcloud compute instance-groups managed add-iam-policy-binding <MIG_NAME> \
  --region=<REGION> \
  --member="serviceAccount:service-<TENANT_PROJECT_NUMBER>@compute-system.iam.gserviceaccount.com" \
  --role="roles/compute.viewer" \
  --project=<MASTER_PROJECT_ID>
```

#### âœ… NEGï¼ˆGKEï¼‰è·¨é¡¹ç›®ç»‘å®š

| NEG ç±»å‹ | è·¨é¡¹ç›®æ”¯æŒ | è¯´æ˜ |
|----------|-----------|------|
| Zonal NEG | âš ï¸ å—é™ | éœ€è¦ Shared VPC + ç‰¹å®š IAM |
| Serverless NEG | âŒ ä¸æ”¯æŒè·¨é¡¹ç›® | åŒé¡¹ç›®é™åˆ¶ |
| Internet NEG | âŒ ä¸é€‚ç”¨ | å¤–éƒ¨åœºæ™¯ |
| **PSC NEG** | âœ… æ¨è | é€šè¿‡ Private Service Connect å®ç°å¼ºéš”ç¦» |

---

### æ–¹æ¡ˆ Bï¼šBackend Service å…¨åœ¨ Master é¡¹ç›®ï¼ˆå½“å‰å®ç°ï¼‰

```
âœ… å®Œå…¨å®˜æ–¹æ”¯æŒï¼Œæ— è·¨é¡¹ç›® API ç»‘å®šé—®é¢˜
âš ï¸ æƒé™é—®é¢˜ï¼šTenant éœ€è¦ä¿®æ”¹è‡ªå·± ILB çš„ URL Map æŒ‡å‘ Master çš„ Backend Service
âš ï¸ è®¡è´¹é—®é¢˜ï¼šCloud Armor è´¹ç”¨å…¨å½’ Master é¡¹ç›®
```

---

## ğŸ“Š ä¸¤ç§æ–¹æ¡ˆæ¶æ„å¯¹æ¯”

```mermaid
graph TD
    subgraph HostProject[Host Project - Shared VPC]
        VPC[Shared VPC Network]
    end

    subgraph TenantProject[Tenant Project]
        ILB["Internal HTTPS LB (INTERNAL_MANAGED)"]
        URLMap[URL Map]
        FwdRule[Forwarding Rule]
        CertA[TLS Certificate]
    end

    subgraph MasterProject[Master Project]
        GKE[GKE Cluster]
        NEG["Zonal NEG (GKE Pods)"]
        MIG["MIG (VM Workload)"]
    end

    subgraph SchemeA[æ–¹æ¡ˆA - Backend in Tenant]
        BS_A["Backend Service (Tenant Project)"]
        CA_A["Cloud Armor Policy (Tenant)"]
        HC_A["Health Check (Tenant)"]
    end

    subgraph SchemeB[æ–¹æ¡ˆB - Backend in Master]
        BS_B["Backend Service (Master Project)"]
        CA_B["Cloud Armor Policy (Master)"]
        HC_B["Health Check (Master)"]
    end

    FwdRule --> URLMap --> BS_A
    FwdRule --> URLMap
    BS_A -->|"è·¨é¡¹ç›®ç»‘å®š IAM Required"| NEG
    BS_A --> CA_A
    BS_A --> HC_A

    URLMap -->|"æ–¹æ¡ˆB"| BS_B
    BS_B --> NEG
    BS_B --> CA_B
    BS_B --> HC_B

    NEG --> GKE
    MIG --> VPC
    GKE --> VPC
```

---

## ğŸ’³ è®¡è´¹æ¶æ„åˆ†æ

### æ–¹æ¡ˆ Aï¼ˆTenant æ‹¥æœ‰ Backend Serviceï¼‰

| è´¹ç”¨é¡¹ | å½’å± | è¯´æ˜ |
|--------|------|------|
| ILB Forwarding Rule | Tenant é¡¹ç›® | æŒ‰è§„åˆ™æ•°è®¡è´¹ |
| Backend Service | Tenant é¡¹ç›® | å…è´¹èµ„æºå¯¹è±¡ |
| **Cloud Armor** | **Tenant é¡¹ç›®** | âœ… å¤©ç„¶éš”ç¦»ï¼ŒTenant è‡ªè¡Œæ‰¿æ‹… |
| å‡ºç«™æµé‡ | Shared VPC Host | å–å†³äºæµé‡è·¯å¾„ |
| Health Check | Tenant é¡¹ç›® | æŒ‰æ¢æµ‹æ¬¡æ•°è®¡è´¹ |

### æ–¹æ¡ˆ Bï¼ˆBackend Service å…¨åœ¨ Masterï¼‰

| è´¹ç”¨é¡¹ | å½’å± | é—®é¢˜ |
|--------|------|------|
| ILB Forwarding Rule | Tenant é¡¹ç›® | âœ… ç‹¬ç«‹ |
| **Cloud Armor** | **Master é¡¹ç›®** | âŒ æ— æ³•ç›´æ¥æ‹†åˆ†è´¦å•åˆ° Tenant |
| Backend Service | Master é¡¹ç›® | æ··åˆåœ¨ä¸€èµ· |

**æ–¹æ¡ˆ B è®¡è´¹æ‹†åˆ†è¡¥æ•‘æ–¹æ¡ˆ**ï¼š

```bash
# é€šè¿‡ Labels æ‰“æ ‡ï¼Œå†ç”¨ Billing Export åˆ° BigQuery è¿›è¡Œè´¹ç”¨å½’å› 
gcloud compute backend-services update <BS_NAME_FOR_TENANT_X> \
  --global \
  --update-labels tenant=tenant-x,env=prod,cost-center=tenant-x-billing \
  --project=<MASTER_PROJECT_ID>

# BigQuery æŸ¥è¯¢ç¤ºä¾‹ï¼ˆbilling export è¡¨ï¼‰
# SELECT labels.value as tenant, SUM(cost) as total_cost
# FROM `billing_dataset.gcp_billing_export`
# WHERE labels.key = 'tenant'
# GROUP BY tenant
```

> âš ï¸ Labels è®¡è´¹æ‹†åˆ†æ˜¯ **è½¯æ€§æ‹†åˆ†**ï¼Œæ— æ³•åœ¨ GCP Console è´¦å•é¡µç›´æ¥ä½“ç°åˆ° Tenant é¡¹ç›®ï¼Œéœ€è¦é€šè¿‡ BigQuery Billing Export äºŒæ¬¡åˆ†æã€‚

---

## ğŸ” æƒé™ç®¡æ§ï¼ˆAuthorizationï¼‰å…³é”®å·®å¼‚

### æ–¹æ¡ˆ A æƒé™çŸ©é˜µ

| æ“ä½œ | æ‰§è¡Œæ–¹ | æ‰€éœ€æƒé™ | é£é™© |
|------|--------|----------|------|
| ä¿®æ”¹ Cloud Armor | Tenant SA | `compute.securityPolicies.*`ï¼ˆTenant é¡¹ç›®ï¼‰ | âœ… ä½ï¼Œç§Ÿæˆ·è‡ªæ²» |
| ç»‘å®šè·¨é¡¹ç›® MIG | Tenant SA | `compute.instanceGroups.use`ï¼ˆMaster é¡¹ç›®ï¼‰ | âš ï¸ éœ€è¦ç²¾ç¡®æˆæƒ |
| ä¿®æ”¹ Health Check | Tenant SA | `compute.healthChecks.*`ï¼ˆTenant é¡¹ç›®ï¼‰ | âœ… ä½ |
| åˆ é™¤ Backend Service | Tenant SA | éœ€é˜²æ­¢è¯¯åˆ å½±å“ LB | âš ï¸ éœ€ OrgPolicy ä¿æŠ¤ |

```bash
# æ–¹æ¡ˆAï¼šåœ¨ Master é¡¹ç›®ç»™ Tenant SA æœ€å°æƒé™ï¼ˆä»…å…è®¸ use MIGï¼Œä¸å…è®¸ deleteï¼‰
gcloud projects add-iam-policy-binding <MASTER_PROJECT_ID> \
  --member="serviceAccount:<TENANT_SA>@<TENANT_PROJECT>.iam.gserviceaccount.com" \
  --role="roles/compute.viewer"

# è‡ªå®šä¹‰ Role ä»…æˆäºˆ use æƒé™
gcloud iam roles create tenantMigUser \
  --project=<MASTER_PROJECT_ID> \
  --permissions="compute.instanceGroups.use,compute.instanceGroups.get"
```

### æ–¹æ¡ˆ B æƒé™çŸ©é˜µ

| æ“ä½œ | æ‰§è¡Œæ–¹ | é£é™© |
|------|--------|------|
| ä¿®æ”¹ Cloud Armor | Master SA / Platform Team | âŒ Tenant æ— æ³•è‡ªä¸»ä¿®æ”¹ï¼Œéœ€é€šè¿‡å¹³å°æ¥å£ |
| ä¿®æ”¹ URL Map | Tenant SA | âš ï¸ éœ€æˆäºˆ Tenant å¯¹ Master é¡¹ç›® URLMap çš„å†™æƒé™ |
| éš”ç¦»æ€§ | â€” | âŒ ä¸€ä¸ª SA è¯¯æ“ä½œå¯èƒ½å½±å“å¤š Tenant |

---

## ğŸ“‹ ä¸‰ç§æ–¹æ¡ˆæ¨ªå‘å¯¹æ¯”ï¼ˆå« PSCï¼‰

| ç»´åº¦ | æ–¹æ¡ˆAï¼šè·¨é¡¹ç›® Backend | æ–¹æ¡ˆBï¼šMaster ç»Ÿä¸€ Backend | æ–¹æ¡ˆCï¼šPSC éš”ç¦» |
|------|----------------------|--------------------------|----------------|
| å®˜æ–¹æ”¯æŒåº¦ | âš ï¸ æœ‰é™æ”¯æŒï¼Œéœ€ IAM é…ç½® | âœ… å®Œå…¨æ”¯æŒ | âœ… å®Œå…¨æ”¯æŒ |
| Cloud Armor å½’å± | âœ… Tenant ç‹¬ç«‹ | âŒ Master ç»Ÿä¸€ | âœ… Tenant ç‹¬ç«‹ |
| è®¡è´¹å¤©ç„¶éš”ç¦» | âœ… æŒ‰é¡¹ç›®è‡ªç„¶éš”ç¦» | âŒ éœ€ Label + BigQuery | âœ… æŒ‰é¡¹ç›®è‡ªç„¶éš”ç¦» |
| Tenant è‡ªæ²»èƒ½åŠ› | âœ… é«˜ | âš ï¸ ä½ï¼Œä¾èµ–å¹³å° | âœ… é«˜ |
| å®ç°å¤æ‚åº¦ | âš ï¸ ä¸­ï¼ˆIAM é…ç½®å¤æ‚ï¼‰ | âœ… ä½ï¼ˆå·²å®ç°ï¼‰ | âŒ é«˜ï¼ˆéœ€ PSC é…ç½®ï¼‰ |
| ç½‘ç»œå®‰å…¨éš”ç¦» | âš ï¸ ä¸­ï¼ˆåŒ VPCï¼‰ | âš ï¸ ä¸­ï¼ˆåŒ VPCï¼‰ | âœ… å¼ºï¼ˆæœåŠ¡è¾¹ç•Œéš”ç¦»ï¼‰ |
| è·¨é¡¹ç›® MIG ç»‘å®š | âš ï¸ æ”¯æŒä½†éä¸»æµ | N/A | N/A |
| è·¨é¡¹ç›® GKE NEG ç»‘å®š | âš ï¸ å—é™ | N/A | âœ… via PSC NEG |

---

## âš ï¸ ç”Ÿäº§ç¯å¢ƒå·²çŸ¥é£é™©

### æ–¹æ¡ˆ A é£é™©ç‚¹

1. **Health Check è·¨é¡¹ç›®å¯è§æ€§**ï¼šHealth Check å¿…é¡»ä¸ Backend Service **åŒé¡¹ç›®**ï¼Œæ— æ³•ç›´æ¥æ¢æµ‹è·¨é¡¹ç›® MIG çš„ç§æœ‰ IPï¼ˆéœ€ç¡®è®¤ Firewall è§„åˆ™æ”¾è¡Œ `35.191.0.0/16` å’Œ `130.211.0.0/22` åˆ° Master é¡¹ç›® VMï¼‰

```bash
# Master é¡¹ç›®å¿…é¡»æ”¾è¡Œ GCP Health Check æ¢é’ˆæº IP
gcloud compute firewall-rules create allow-health-check-from-tenant-lb \
  --network=<SHARED_VPC_NETWORK> \
  --action=ALLOW \
  --rules=tcp:<BACKEND_PORT> \
  --source-ranges=35.191.0.0/16,130.211.0.0/22 \
  --project=<MASTER_PROJECT_ID>
```

2. **è·¨é¡¹ç›® NEG ç»‘å®š GKE**ï¼šGKE NEG é€šå¸¸åªèƒ½è¢«**åŒé¡¹ç›®**çš„ Backend Service ç›´æ¥å¼•ç”¨ï¼Œè·¨é¡¹ç›®å¼•ç”¨éœ€è¦é€šè¿‡ **PSC** æˆ–è€…å°† NEG åˆ›å»ºåŠ¨ä½œå§”æ‰˜ç»™ Master é¡¹ç›®

3. **Tenant SA éœ€è¦ `compute.instances.admin`**ï¼šä½ æåˆ°å·²å‘ç°è¿™ä¸ªé—®é¢˜ï¼Œè¿™ä¸ªæƒé™è¿‡å¤§ï¼Œåº”é€šè¿‡**è‡ªå®šä¹‰ Role** ç¼©å°æƒé™é¢

### æ–¹æ¡ˆ B é£é™©ç‚¹

1. Master é¡¹ç›® SA è¯¯æ“ä½œå½±å“èŒƒå›´æ‰©å¤§
2. Cloud Armor è§„åˆ™å˜æ›´éœ€è¦å¹³å°å›¢é˜Ÿä»‹å…¥ï¼ŒTenant è‡ªæ²»æ€§å·®
3. è®¡è´¹æ— æ³•åœ¨é¡¹ç›®ç»´åº¦è‡ªç„¶æ‹†åˆ†

---

## ğŸ† æ¨èç»“è®º

```
çŸ­æœŸï¼ˆå·²å®ç°æ–¹æ¡ˆ Bï¼‰ï¼š
  â†’ é€šè¿‡ Label + Billing Export BigQuery å®ç°è½¯æ€§è®¡è´¹æ‹†åˆ†
  â†’ ä¸ºæ¯ä¸ª Tenant çš„ Backend Service ç»Ÿä¸€æ‰“ tenant=xxx æ ‡ç­¾
  â†’ ç”¨è‡ªå®šä¹‰ IAM Role æ”¶ç´§ Tenant å¯¹ Master é¡¹ç›®çš„æ“ä½œæƒé™

ä¸­æœŸï¼ˆæ¼”è¿›åˆ°æ–¹æ¡ˆ Aï¼‰ï¼š
  â†’ ä½¿ç”¨ Shared VPC + è·¨é¡¹ç›® MIG ç»‘å®šï¼ˆé…åˆç²¾ç¡® IAM Custom Roleï¼‰
  â†’ Health Check æ”¾åœ¨ Tenant é¡¹ç›®ï¼ŒFirewall è§„åˆ™åœ¨ Master é¡¹ç›®æ”¾è¡Œ
  â†’ Cloud Armor è®¡è´¹å¤©ç„¶å½’ Tenant é¡¹ç›®

é•¿æœŸï¼ˆè§„æ¨¡åŒ–å¤šç§Ÿæˆ·ï¼‰ï¼š
  â†’ è€ƒè™‘ Private Service Connectï¼ˆPSCï¼‰
  â†’ Master é¡¹ç›®å‘å¸ƒ PSC Service Attachment
  â†’ Tenant é¡¹ç›®é€šè¿‡ PSC NEG æ¥å…¥ï¼Œå®ç°æœåŠ¡è¾¹ç•Œçº§éš”ç¦»
  â†’ å®Œå…¨è§£è€¦ä¸¤ä¸ªé¡¹ç›®çš„ç”Ÿå‘½å‘¨æœŸä¾èµ–
```

---

## ğŸ”§ æ–¹æ¡ˆ A è½åœ°æœ€å° IAM é…ç½®å‚è€ƒ

```bash
# 1. åˆ›å»ºè‡ªå®šä¹‰ Roleï¼ˆMaster é¡¹ç›®ï¼‰ï¼Œä»…å…è®¸ Tenant å¼•ç”¨ MIG/NEGï¼Œä¸å…è®¸åˆ é™¤
gcloud iam roles create crossProjectBackendUser \
  --project=<MASTER_PROJECT_ID> \
  --title="Cross Project Backend User" \
  --permissions="compute.instanceGroups.use,compute.instanceGroups.get,compute.networkEndpointGroups.use,compute.networkEndpointGroups.get,compute.instances.get"

# 2. æˆæƒ Tenant é¡¹ç›®çš„ LB Service Agent
gcloud projects add-iam-policy-binding <MASTER_PROJECT_ID> \
  --member="serviceAccount:service-<TENANT_PROJECT_NUMBER>@compute-system.iam.gserviceaccount.com" \
  --role="projects/<MASTER_PROJECT_ID>/roles/crossProjectBackendUser"

# 3. åŒæ—¶æˆæƒ compute.networkViewer ä»¥ä¾¿ LB è§£æ Shared VPC ç½‘ç»œ
gcloud projects add-iam-policy-binding <MASTER_PROJECT_ID> \
  --member="serviceAccount:service-<TENANT_PROJECT_NUMBER>@compute-system.iam.gserviceaccount.com" \
  --role="roles/compute.networkViewer"
```

---

**æ ¸å¿ƒç»“è®º**ï¼š
- æ–¹æ¡ˆ Aï¼ˆTenant æ‹¥æœ‰ Backend + è·¨é¡¹ç›® MIGï¼‰**æŠ€æœ¯å¯è¡Œä½† IAM é…ç½®å¤æ‚**ï¼ŒCloud Armor å’Œè®¡è´¹å¤©ç„¶éš”ç¦»ï¼Œæ˜¯é•¿æœŸæ¨èæ–¹å‘
- æ–¹æ¡ˆ Bï¼ˆå½“å‰å®ç°ï¼‰**å·¥ç¨‹æœ€ç®€å•**ï¼Œä½†è®¡è´¹å’Œæƒé™éš”ç¦»éœ€è¦é¢å¤–å·¥ç¨‹ä¿éšœ
- å¦‚æœä½ çš„æ ¸å¿ƒè¯‰æ±‚æ˜¯ **Tenant ç‹¬ç«‹è®¡è´¹ + Cloud Armor è‡ªæ²»**ï¼Œåº”ä¼˜å…ˆæ¨è¿›æ–¹æ¡ˆ A æˆ– PSC æ–¹æ¡ˆ


# Gemini

This is a classic "Platform vs. Tenant" architectural challenge. In a Shared VPC environment, Google Cloud provides several paths, but the choice between Cross-Project Backend Attachment and Private Service Connect (PSC) hinges on how much "trust" you have between the Master and Tenant projects.
As your Technical Partner, here is the architectural breakdown and feasibility confirmation.
1. Feasibility Confirmation
Is it officially supported?
Yes. In a Shared VPC environment, a Regional Internal HTTP(S) Load Balancer (Project A) can officially reference Backends (NEGs or MIGs) located in Project B, provided both projects are service projects attached to the same Shared VPC Host Project.
Implementation Logic
 * The Network context: Since both projects share the same VPC, the underlying IP reachability is native.
 * The Resource context: The Backend Service (in Tenant Project) points to the self_link of the NEG or MIG (in Master Project).
 * The Region constraint: The ILB, Backend Service, and the NEGs/MIGs must reside in the same GCP Region. Cross-region backend binding is not supported for Internal HTTPS LBs.
2. Architecture Comparison: Option A vs. Option B vs. PSC
Based on your requirements for independent billing and Tenant-level Cloud Armor, here is how the options stack up:
| Feature | Option A: Backend Service in Tenant (Your Current) | Option B: Backend Service in Master | Option C: Private Service Connect (Recommended) |
|---|---|---|---|
| Cloud Armor Policy | Owned by Tenant | Owned by Master (Hard to split) | Owned by Tenant |
| Billing (LB + WAF) | Tenant Project | Master Project | Tenant Project |
| IAM Complexity | High (Cross-project perms) | Low | Medium |
| Security Isolation | Network-level (Shared VPC) | Shared | Consumer-Provider Abstraction |
| Scalability | Good | Hard to manage at scale | Best (Cleanest handoff) |
Why Private Service Connect (PSC) might be your "North Star"
While your current cross-project binding works, PSC is the Google-recommended way for multi-tenant platforms.
 * Producer (Master): You create a "Service Attachment" pointing to your GKE internal load balancer.
 * Consumer (Tenant): The tenant creates a PSC NEG in their project.
 * Result: The Tenantâ€™s ILB points to the PSC NEG. The Tenant pays for their own ILB and Cloud Armor. You (Master) only pay for the GKE resources. No cross-project IAM on MIGs/NEGs is required.
3. Deep Dive: Cross-Project IAM & Permissions
If you stick with Option A (Backend Service in Tenant), you must solve the "Permission Leakage" issue you mentioned.
Required Roles
To allow the Tenant Project to "see" and "use" the Master Project's backends:
 * LB Service Agent: The service-PROJECT_NUMBER@gcp-sa-compute.iam.gserviceaccount.com in the Tenant Project needs:
   * roles/compute.networkUser on the Shared VPC Host subnet.
   * roles/compute.loadBalancerServiceUser on the Master Project (to bind NEGs).
 * Automation/User Identity: The principal creating the Backend Service needs compute.instances.use or compute.networkEndpointGroups.use on the Master resources.
> Risk Warning: Giving a Tenant Project Service Account compute.instanceAdmin on the Master project is a critical security risk. You should use a custom role or strictly scoped compute.loadBalancerServiceUser to ensure the Tenant cannot delete or modify your GKE nodes.
> 
4. Billing & Cost Attribution
In Option A (Tenant-side ILB/Backend Service):
 * Load Balancing Fees: Charged to the Tenant Project.
 * Cloud Armor Fees: Charged to the Tenant Project.
 * Data Processing: Charged to the project where the LB is defined (Tenant).
 * Compute (GKE Nodes): Charged to the Master Project.
This aligns perfectly with your goal of independent billing for security and traffic-heavy features (WAF).
5. Summary & Recommendation
Which Backend type?
 * Use NEGs (Network Endpoint Groups): Since you are on GKE, standalone NEGs are superior to MIGs. They allow container-native load balancing (skipping the NodePort hop), reducing latency and simplifying health checks.
Final Verdict
 * Immediate Fix (Current Path): Continue with Cross-Project NEG binding. Refine IAM: Remove instanceAdmin and replace it with compute.loadBalancerServiceUser at the project level, or better, granted only on specific NEGs via IAM conditions.
 * Structural Improvement (Long-term): Move to Private Service Connect (PSC). It removes the need for the Tenant to have any direct IAM visibility into your Master Project. The interface becomes a "Service Attachment" URL.
Next Step
Would you like me to provide the Terraform/YAML manifest for setting up a PSC Service Attachment in the Master project and a PSC NEG in the Tenant project to see how that simplifies your security boundary?



# Cross-Project Backend Service ç»‘å®š Master MIG å¯è¡Œæ€§åˆ†æ

## æ¾„æ¸…ä½ çš„æ¶æ„æ„å›¾

```mermaid
graph LR
    subgraph Tenant Project
        CA[Cloud Armor WAF]
        ILB[HTTPS ILB]
        BS[Backend Service]
        CA --> ILB --> BS
    end
    subgraph Master Project
        MIG[Managed Instance Group]
        GKE[GKE Nodes]
        MIG --> GKE
    end
    BS -->|Cross-Project ç›´æ¥ç»‘å®š?| MIG
```

---

## ç»“è®ºï¼š**ä¸å¯è¡Œï¼ˆç¡¬é™åˆ¶ï¼‰**

> GCP Backend Service **åªèƒ½ç»‘å®šåŒ Project å†…çš„ Instance Groupï¼ˆMIG/UIGï¼‰**ã€‚  
> è¿™ä¸æ˜¯æƒé™é—®é¢˜ï¼Œæ˜¯ GCP èµ„æºå¼•ç”¨æ¨¡å‹çš„ç¡¬çº¦æŸã€‚

éªŒè¯ä¸€ä¸‹ï¼š
```bash
# å°è¯•è·¨ Project æ·»åŠ  MIG åˆ° Backend Service ä¼šç›´æ¥æŠ¥é”™
gcloud compute backend-services add-backend TENANT_BS \
    --instance-group=projects/MASTER_PROJECT/zones/ZONE/instanceGroups/master-mig \
    --project=TENANT_PROJECT_ID

# æŠ¥é”™ï¼š
# ERROR: Invalid value for field 'resource.backends[0].group':
# 'projects/MASTER_PROJECT/...' Backend must be in the same project
```

---

## ä½†ä½ è¯´çš„ã€ŒCross-Project Backendã€æ˜¯å¦ä¸€å›äº‹

```mermaid
graph LR
    subgraph Tenant Project
        ILB[HTTPS ILB Forwarding Rule]
        BS_T[Backend Service - Tenant è‡ªå·±çš„]
    end
    subgraph Master Project
        BS_M[Backend Service - Master çš„]
        MIG[MIG]
        BS_M --> MIG
    end
    ILB -->|routing rule cross-project| BS_M
    ILB --> BS_T
```

**GCP æ”¯æŒçš„ Cross-Project æ˜¯ï¼š**  
ILB çš„ **URL Map routing rule** ç›´æ¥å¼•ç”¨å¦ä¸€ä¸ª Project çš„ **Backend Service**  
â†’ ä½† Cloud Armor Policy æ˜¯ç»‘åœ¨ **Tenant è‡ªå·±çš„ Backend Service** ä¸Šçš„  
â†’ æµé‡èµ°åˆ° Master çš„ Backend Service æ—¶ï¼Œ**Cloud Armor ä¸ä¼šç”Ÿæ•ˆ**

---

## ä½ çš„æ ¸å¿ƒè¯‰æ±‚ vs å®é™…é™åˆ¶

| è¯‰æ±‚ | æ˜¯å¦å¯è¡Œ |
|------|---------|
| Cloud Armor åœ¨ Tenant Project | âœ… |
| Backend Service åœ¨ Tenant Project | âœ… |
| Tenant BS ç›´æ¥ç»‘å®š Master MIG | âŒ ç¡¬é™åˆ¶ |
| ILB routing åˆ° Master BSï¼ˆCloud Armor å¤±æ•ˆï¼‰ | âœ… ä½†ä¸æ»¡è¶³ WAF éœ€æ±‚ |

---

## çœŸæ­£å¯è¡Œçš„æ–¹æ¡ˆï¼šCross-Project NEGï¼ˆé MIGï¼‰

```mermaid
graph LR
    subgraph Tenant Project
        CA[Cloud Armor]
        ILB[HTTPS ILB]
        BS[Backend Service]
        CA --> BS
        ILB --> BS
    end
    subgraph Master Project
        NEG[Zonal NEG - æŒ‡å‘ GKE Pod/VM IP:Port]
        GKE[GKE Workload]
        NEG -.->|endpoint| GKE
    end
    BS -->|âœ… æ”¯æŒè·¨ Project| NEG
```

**å…³é”®åŒºåˆ«ï¼šMIG ä¸èƒ½è·¨ Projectï¼Œä½† NEG å¯ä»¥ã€‚**

```bash
# Master Project åˆ›å»º NEG
gcloud compute network-endpoint-groups create master-workload-neg \
    --network-endpoint-type=GCE_VM_IP_PORT \
    --zone=ZONE \
    --project=MASTER_PROJECT_ID

# æ·»åŠ  GKE Node/Pod endpoints
gcloud compute network-endpoint-groups update master-workload-neg \
    --add-endpoint='instance=INSTANCE_NAME,ip=POD_IP,port=8080' \
    --zone=ZONE \
    --project=MASTER_PROJECT_ID

# Tenant Project Backend Service ç»‘å®š Master NEG âœ…
gcloud compute backend-services add-backend TENANT_BS \
    --network-endpoint-group=projects/MASTER_PROJECT_ID/zones/ZONE/networkEndpointGroups/master-workload-neg \
    --network-endpoint-group-zone=ZONE \
    --project=TENANT_PROJECT_ID \
    --global
```

---

## å¦‚æœä½ çš„ Master å¿…é¡»ç”¨ MIGï¼Œå”¯ä¸€è·¯å¾„

```mermaid
graph LR
    subgraph Tenant Project
        CA[Cloud Armor]
        BS_T[Backend Service]
        CA --> BS_T
    end
    subgraph Master Project
        MIG[MIG]
        ILB_M[Internal LB - passthrough]
        MIG --> ILB_M
        NEG_PSC[PSC / ILB IP ä½œä¸º Serverless NEG]
    end
    BS_T -->|Internet NEG æˆ– PSC NEG| NEG_PSC
    NEG_PSC --> ILB_M
```

åœ¨ Tenant BS é‡Œç”¨ **Private Service Connect NEG** æˆ– **Hybrid NEG** æŒ‡å‘ Master çš„ Internal LB IPï¼Œé—´æ¥æ‰“é€šï¼ŒCloud Armor ä¾ç„¶åœ¨ Tenant ä¾§ç”Ÿæ•ˆã€‚

---

## æ€»ç»“å»ºè®®

**å¦‚æœ Master æ˜¯ GKE** â†’ æ”¹ç”¨ Cross-Project Zonal NEGï¼Œå®Œå…¨æ»¡è¶³ä½ çš„éœ€æ±‚  
**å¦‚æœ Master å¿…é¡»æ˜¯ MIG** â†’ PSC NEG é—´æ¥ç»‘å®šï¼Œå¤šä¸€è·³ä½† WAF ä¸å—å½±å“


# Cross-Project Zonal NEG æ–¹æ¡ˆç»†åŒ–

## å‰ææ¡ä»¶ç¡®è®¤

```mermaid
graph TD
    A[å‰ææ£€æŸ¥] --> B[Shared VPC å·²é…ç½®]
    A --> C[Master GKE é›†ç¾¤å·²å­˜åœ¨]
    A --> D[Tenant HTTPS ILB + Cloud Armor å·²å­˜åœ¨]
    B --> E{NEG ç±»å‹é€‰æ‹©}
    E -->|GKE Pod ç›´è¿| F[GKE_VM_IP_PORT NEG]
    E -->|GKE Service ç«¯å£| G[Standalone Zonal NEG]
```

---

## æ•´ä½“æ¶æ„

```mermaid
graph LR
    subgraph Tenant Project
        CA[Cloud Armor Policy]
        BS[Backend Service]
        HC[Health Check]
        ILB[HTTPS ILB URL Map]
        FW_RULE[Forwarding Rule VIP]
        CA --> BS
        HC --> BS
        BS --> ILB
        ILB --> FW_RULE
    end

    subgraph Master Project
        NEG[Zonal NEG GKE_VM_IP_PORT]
        GKE_NODE[GKE Node VM]
        POD[Pod :8080]
        NEG -.->|endpoint: NodeIP:NodePort| GKE_NODE
        GKE_NODE --> POD
    end

    BS -->|Cross-Project NEG| NEG
    FW_RULE -->|Client Request| CA
```

---

## è¯¦ç»†å®æ–½æ­¥éª¤

### Step 1ï¼šMaster Project - å‡†å¤‡ GKE Service æš´éœ²æ–¹å¼

GKE ä¾§éœ€è¦è®©æµé‡å¯ä»¥é€šè¿‡ **NodePort** æˆ– **Pod IP ç›´è¿** è¿›æ¥ï¼š

```yaml
# Master Project - K8s Service ä½¿ç”¨ NodePort
apiVersion: v1
kind: Service
metadata:
  name: master-workload-svc
  namespace: default
spec:
  type: NodePort
  selector:
    app: master-workload
  ports:
    - port: 80
      targetPort: 8080
      nodePort: 30080   # å›ºå®š NodePortï¼Œä¾¿äº NEG é…ç½®
```

```bash
kubectl apply -f service.yaml --context=MASTER_GKE_CONTEXT
```

---

### Step 2ï¼šMaster Project - åˆ›å»º Standalone Zonal NEG

```bash
# åˆ›å»º NEGï¼Œç½‘ç»œæŒ‡å‘ Shared VPC
gcloud compute network-endpoint-groups create master-gke-neg \
    --network-endpoint-type=GCE_VM_IP_PORT \
    --zone=asia-east1-b \
    --network=projects/HOST_PROJECT_ID/global/networks/SHARED_VPC_NAME \
    --subnetwork=projects/HOST_PROJECT_ID/regions/asia-east1/subnetworks/SUBNET_NAME \
    --project=MASTER_PROJECT_ID

# æŸ¥çœ‹ GKE Node å®ä¾‹å
kubectl get nodes -o wide --context=MASTER_GKE_CONTEXT
# è®°å½• NODE_NAME å’Œ INTERNAL_IP

# å°† GKE Node åŠ å…¥ NEGï¼ˆNodePort æ–¹å¼ï¼‰
gcloud compute network-endpoint-groups update master-gke-neg \
    --add-endpoint='instance=GKE_NODE_INSTANCE_NAME,ip=NODE_INTERNAL_IP,port=30080' \
    --zone=asia-east1-b \
    --project=MASTER_PROJECT_ID

# å¤šèŠ‚ç‚¹å…¨éƒ¨åŠ å…¥
gcloud compute network-endpoint-groups update master-gke-neg \
    --add-endpoint='instance=GKE_NODE_2,ip=NODE_2_IP,port=30080' \
    --zone=asia-east1-b \
    --project=MASTER_PROJECT_ID
```

> âš ï¸ GKE èŠ‚ç‚¹ Auto Scaling æ—¶ï¼Œéœ€è¦è‡ªåŠ¨åŒæ­¥ NEG endpointsï¼Œè§ Step 6ã€‚

---

### Step 3ï¼šIAM æˆæƒ - Tenant å¯ä»¥ä½¿ç”¨ Master NEG

```bash
# æ–¹å¼ä¸€ï¼šç²¾ç»†åŒ–åˆ°å…·ä½“ NEG èµ„æºï¼ˆæ¨èï¼‰
gcloud compute network-endpoint-groups add-iam-policy-binding master-gke-neg \
    --member="serviceAccount:TENANT_COMPUTE_SA@TENANT_PROJECT.iam.gserviceaccount.com" \
    --role="roles/compute.networkEndpointGroupAdmin" \
    --zone=asia-east1-b \
    --project=MASTER_PROJECT_ID

# æ–¹å¼äºŒï¼šProject çº§åˆ«ï¼ˆæƒé™è¾ƒå¤§ï¼Œä¸æ¨èç”Ÿäº§ï¼‰
gcloud projects add-iam-policy-binding MASTER_PROJECT_ID \
    --member="serviceAccount:TENANT_COMPUTE_SA@TENANT_PROJECT.iam.gserviceaccount.com" \
    --role="roles/compute.networkViewer"
```

---

### Step 4ï¼šTenant Project - Backend Service ç»‘å®š Master NEG

```bash
# 4.1 åˆ›å»º Health Checkï¼ˆåœ¨ Tenant Projectï¼Œæ¢æµ‹ Master GKE NodePortï¼‰
gcloud compute health-checks create http master-neg-hc \
    --port=30080 \
    --request-path=/healthz \
    --check-interval=10s \
    --timeout=5s \
    --healthy-threshold=2 \
    --unhealthy-threshold=3 \
    --project=TENANT_PROJECT_ID \
    --global

# 4.2 åˆ›å»º Backend Service å¹¶ç»‘å®š Cloud Armor
gcloud compute backend-services create tenant-bs-master \
    --protocol=HTTP \
    --health-checks=master-neg-hc \
    --security-policy=YOUR_CLOUD_ARMOR_POLICY \
    --global \
    --project=TENANT_PROJECT_ID

# 4.3 ç»‘å®š Master Project çš„ NEG âœ…
gcloud compute backend-services add-backend tenant-bs-master \
    --network-endpoint-group=projects/MASTER_PROJECT_ID/zones/asia-east1-b/networkEndpointGroups/master-gke-neg \
    --network-endpoint-group-zone=asia-east1-b \
    --balancing-mode=RATE \
    --max-rate-per-endpoint=100 \
    --global \
    --project=TENANT_PROJECT_ID
```

---

### Step 5ï¼šé˜²ç«å¢™è§„åˆ™ - å…è®¸ Health Check å’Œæµé‡è¿›å…¥ Master GKE

```bash
# Health Check æº IP èŒƒå›´ï¼ˆGCP å›ºå®šï¼‰
gcloud compute firewall-rules create allow-tenant-hc-to-master-gke \
    --network=SHARED_VPC_NAME \
    --allow=tcp:30080 \
    --source-ranges=130.211.0.0/22,35.191.0.0/16 \
    --target-tags=gke-MASTER_CLUSTER_NAME-node \
    --project=HOST_PROJECT_ID \   # é˜²ç«å¢™è§„åˆ™åœ¨ Host Project
    --description="Allow GCP HC to Master GKE NodePort"

# å…è®¸ ILB Proxy å­ç½‘æµé‡åˆ° Master GKE
gcloud compute firewall-rules create allow-tenant-ilb-to-master-gke \
    --network=SHARED_VPC_NAME \
    --allow=tcp:30080 \
    --source-ranges=PROXY_SUBNET_CIDR \
    --target-tags=gke-MASTER_CLUSTER_NAME-node \
    --project=HOST_PROJECT_ID
```

---

### Step 6ï¼šNode åŠ¨æ€æ‰©ç¼©å®¹æ—¶ NEG è‡ªåŠ¨åŒæ­¥

GKE èŠ‚ç‚¹å˜åŒ–æ—¶éœ€è¦è‡ªåŠ¨æ›´æ–° NEGï¼Œæ¨èä½¿ç”¨ **GKE + Custom Controller** æˆ–è„šæœ¬ï¼š

```bash
# æ–¹æ¡ˆï¼šGKE Node åŠ  labelï¼Œé…åˆ DaemonSet è‡ªæ³¨å†Œï¼ˆç®€åŒ–ç‰ˆç”¨è„šæœ¬ï¼‰
# ç›‘å¬ GKE Node å˜åŒ–ï¼Œè‡ªåŠ¨ add/remove NEG endpoint

#!/bin/bash
# sync-neg-endpoints.sh

ZONE="asia-east1-b"
NEG_NAME="master-gke-neg"
MASTER_PROJECT="MASTER_PROJECT_ID"
NODE_PORT="30080"

# è·å–å½“å‰æ‰€æœ‰ Ready Node
CURRENT_NODES=$(kubectl get nodes --context=MASTER_GKE_CONTEXT \
    -o jsonpath='{range .items[*]}{.metadata.name},{.status.addresses[?(@.type=="InternalIP")].address}{"\n"}{end}')

# è·å–å½“å‰ NEG ä¸­çš„ endpoints
CURRENT_NEG=$(gcloud compute network-endpoint-groups list-network-endpoints $NEG_NAME \
    --zone=$ZONE --project=$MASTER_PROJECT \
    --format="value(networkEndpoint.instance,networkEndpoint.ipAddress)")

echo "$CURRENT_NODES" | while IFS=',' read -r name ip; do
    gcloud compute network-endpoint-groups update $NEG_NAME \
        --add-endpoint="instance=$name,ip=$ip,port=$NODE_PORT" \
        --zone=$ZONE \
        --project=$MASTER_PROJECT 2>/dev/null
done
```

> ğŸ’¡ **æ›´ä¼˜æ–¹æ¡ˆ**ï¼šä½¿ç”¨ GKE Ingress with Container-native Load Balancingï¼ŒGKE ä¼šè‡ªåŠ¨ç®¡ç† NEG endpointsï¼Œä½†è¿™éœ€è¦ GKE ä¾§ä¹Ÿåšå¯¹åº”é…ç½®ã€‚

---

## å®Œæ•´æµé‡è·¯å¾„éªŒè¯

```mermaid
sequenceDiagram
    participant Client
    participant FWD as Forwarding Rule (Tenant VIP)
    participant PROXY as Envoy Proxy (Tenant)
    participant CA as Cloud Armor (Tenant)
    participant BS as Backend Service (Tenant)
    participant NEG as Zonal NEG (Master)
    participant NODE as GKE Node (Master)
    participant POD as Pod (Master)

    Client->>FWD: HTTPS Request
    FWD->>PROXY: TLS Termination
    PROXY->>CA: WAF æ£€æŸ¥
    CA-->>PROXY: Allow/Deny
    PROXY->>BS: è½¬å‘è¯·æ±‚
    BS->>NEG: é€‰æ‹© Endpoint
    NEG->>NODE: NodeIP:NodePort
    NODE->>POD: kube-proxy è½¬å‘
    POD-->>Client: Response
```

---

## å…³é”®æ³¨æ„äº‹é¡¹

| é¡¹ç›® | æ³¨æ„ç‚¹ |
|------|--------|
| NEG ä¸ GKE åŒ Zone | NEG å¿…é¡»åœ¨ GKE Node ç›¸åŒçš„ Zoneï¼Œå¤š Zone éœ€åˆ›å»ºå¤šä¸ª NEG |
| Health Check ç«¯å£ | éœ€è¦ä¸ NodePort ä¸€è‡´ï¼Œä¸”é˜²ç«å¢™æ”¾é€š HC æº IP |
| GKE Node Autoscaler | èŠ‚ç‚¹æ‰©ç¼©å®¹éœ€è¦åŒæ­¥æ›´æ–° NEG endpoints |
| Shared VPC å­ç½‘ | NEG åˆ›å»ºæ—¶å¿…é¡»æŒ‡å®š Shared VPC çš„ subnet |
| Cloud Armor ç”Ÿæ•ˆä½ç½® | ç»‘å®šåœ¨ Tenant BS ä¸Šï¼Œåœ¨æµé‡è¿›å…¥ NEG ä¹‹å‰å·²ç”Ÿæ•ˆ âœ… |
| å¤š Zone é«˜å¯ç”¨ | æ¯ä¸ª Zone åˆ›å»ºç‹¬ç«‹ NEGï¼ŒBS ç»‘å®šæ‰€æœ‰ Zone çš„ NEG |